"""
Jarvis V7.0 Configuration

ç»Ÿä¸€é…ç½®ä¸­å¿ƒï¼Œæ”¯æŒï¼š
- å¤š LLM Provider (OpenAI, Ollama, Gemini)
- è§’è‰²åˆ‡æ¢ (default, smart, coder, fast, vision)
- äººæ ¼ Prompt å®šåˆ¶
- ä»£ç†è®¾ç½®
"""

import os
from dotenv import load_dotenv

# åŠ è½½ .env æ–‡ä»¶
load_dotenv()


class Config:
    """Jarvis ç»Ÿä¸€é…ç½®ç±»"""
    
    # =========================================
    # ğŸ­ äººæ ¼é…ç½®ç³»ç»Ÿ (Personality System)
    # =========================================
    # æ”¯æŒè¯­éŸ³/æ–‡å­—æ¨¡å¼çš„å·®å¼‚åŒ–äººæ ¼ï¼Œä»¥åŠè§’è‰²ç‰¹å®šçš„è¡Œä¸ºçº¦æŸ
    
    PERSONALITY = {
        # åŸºç¡€äººæ ¼ï¼ˆæ‰€æœ‰æ¨¡å¼å…±äº«ï¼‰
        "base": {
            "name": os.getenv("JARVIS_ASSISTANT_NAME", "Jarvis"),
            "trait": "ç®€æ´ã€ä¸“ä¸šã€å‹å¥½",
            "language": "ä¸­æ–‡",
        },
        
        # è¯­éŸ³æ¨¡å¼çº¦æŸï¼ˆè¢«æœ—è¯»å‡ºæ¥ï¼Œå¿…é¡»ç®€æ´å£è¯­åŒ–ï¼‰
        "voice_mode": {
            "style": "æåº¦ç®€æ´ï¼Œ1-2å¥è¯è§£å†³é—®é¢˜ï¼Œåƒæœ‹å‹èŠå¤©",
            "rules": [
                "ä¸è¦é•¿ç¯‡å¤§è®ºï¼Œç”¨æˆ·åœ¨å¬ä¸æ˜¯åœ¨çœ‹",
                "ä¸è¦ä½¿ç”¨ markdownã€åˆ—è¡¨ã€ä»£ç å—",
                "ä¸è¦åˆ†æè¿‡ç¨‹ï¼Œç›´æ¥ç»™ç»“æœ",
                "ä¸è¦åé—®ï¼Œé™¤éçœŸçš„éœ€è¦æ¾„æ¸…",
            ],
            "example_bad": "æˆ‘çœ‹åˆ°æ‚¨çš„å±å¹•ä¸Šæ˜¾ç¤ºçš„æ˜¯ä¸€ä¸ªä»£ç ç¼–è¾‘å™¨ï¼Œå¯èƒ½æ˜¯ VS Codeï¼Œå¹¶ä¸”æ‚¨åˆšåˆšæ‰§è¡Œäº†ä¸€ä¸ªåˆ‡æ¢æ¨¡å‹çš„æ“ä½œ...",
            "example_good": "å±å¹•ä¸Šæ˜¯ VS Codeï¼Œæ‰“å¼€äº† main.pyã€‚",
        },
        
        # æ–‡å­—æ¨¡å¼çº¦æŸï¼ˆå¯ä»¥é€‚å½“è¯¦ç»†ï¼‰
        "text_mode": {
            "style": "æ¸…æ™°å‡†ç¡®ï¼Œå¯ä»¥é€‚å½“è¯¦ç»†ï¼Œæ”¯æŒ markdown",
            "rules": [
                "å¯ä»¥ä½¿ç”¨æ ¼å¼åŒ–æé«˜å¯è¯»æ€§",
                "å¤æ‚é—®é¢˜å¯ä»¥åˆ†æ­¥éª¤è§£é‡Š",
            ],
        },
        
        # è§’è‰²ç‰¹å®šäººæ ¼è¡¥å……
        "roles": {
            "default": "å¹³è¡¡é€šç”¨ï¼Œæ—¥å¸¸å¯¹è¯å’Œä»»åŠ¡æ‰§è¡Œ",
            "smart": "æ·±åº¦æ€è€ƒï¼Œä½†ä»ä¿æŒç®€æ´ï¼Œé€‚åˆå¤æ‚æ¨ç†",
            "coder": "æŠ€æœ¯ç²¾å‡†ï¼Œä»£ç ä¼˜å…ˆï¼Œå°‘åºŸè¯",
            "vision": "æè¿°æ‰€è§å³å¯ï¼Œä¸è¦è¿‡åº¦åˆ†æå’Œæ¨æµ‹",
            "fast": "æé€Ÿå“åº”ï¼Œä¸€å¥è¯æå®š",
        },
    }
    
    # å…¼å®¹æ—§ç‰ˆï¼šä¿ç•™ PERSONALITY_PROMPTï¼ˆä»æ–°é…ç½®ç”Ÿæˆï¼‰
    @classmethod
    def get_personality_prompt(cls) -> str:
        """è·å–åŸºç¡€äººæ ¼ Promptï¼ˆå…¼å®¹æ—§ä»£ç ï¼‰"""
        base = cls.PERSONALITY.get("base", {})
        return f"""ä½ æ˜¯ {base.get('name', 'Jarvis')}ï¼Œä¸€ä¸ªæ™ºèƒ½ AI åŠ©æ‰‹ã€‚
ä½ çš„ç‰¹ç‚¹ï¼š{base.get('trait', 'ç®€æ´ã€ä¸“ä¸šã€å‹å¥½')}
ä½¿ç”¨{base.get('language', 'ä¸­æ–‡')}ä¸ç”¨æˆ·äº¤æµã€‚"""
    
    # ä¿æŒå‘åå…¼å®¹
    PERSONALITY_PROMPT = property(lambda self: Config.get_personality_prompt())
    
    # ç”¨æˆ·è‡ªå®šä¹‰åç§°ï¼ˆç”¨äºä¸ªæ€§åŒ–ç§°å‘¼ï¼‰
    USER_NAME = os.getenv("JARVIS_USER_NAME", "ä¸»äºº")
    
    # åŠ©æ‰‹åç§°
    ASSISTANT_NAME = os.getenv("JARVIS_ASSISTANT_NAME", "Jarvis")
    
    # =========================================
    # ğŸŒ ç½‘ç»œä¸ä»£ç†é…ç½®
    # =========================================
    PROXY_ENABLED = os.getenv("PROXY_ENABLED", "false").lower() == "true"
    PROXY_URL = os.getenv("PROXY_URL", "http://127.0.0.1:7897")
    
    # =========================================
    # ğŸ¤ è¯­éŸ³ä¸å”¤é†’è¯é…ç½®
    # =========================================
    PICOVOICE_ACCESS_KEY = os.getenv("PICOVOICE_ACCESS_KEY")
    USE_BUILTIN_KEYWORD = os.getenv("USE_BUILTIN_KEYWORD", "true").lower() == "true"
    WAKE_WORD_FILE = os.getenv("WAKE_WORD_FILE", "jarvis.ppn")
    TTS_VOICE = os.getenv("TTS_VOICE", "zh-CN-XiaoxiaoNeural")
    
    # å”¤é†’è¯çµæ•åº¦ (0.0 - 1.0)
    try:
        WAKE_SENSITIVITY = float(os.getenv("WAKE_SENSITIVITY", "0.7"))
        WAKE_SENSITIVITY = max(0.0, min(1.0, WAKE_SENSITIVITY))
    except Exception:
        WAKE_SENSITIVITY = 0.7

    # =========================================
    # ğŸ¤– LLM è§’è‰²é…ç½® (V7.0 ç»Ÿä¸€æ¶æ„)
    # =========================================
    # æ”¯æŒçš„ provider: "openai", "ollama", "gemini"
    # 
    # è§’è‰²è¯´æ˜:
    # - default: å¹³è¡¡æ¨¡å¼ï¼Œæ—¥å¸¸å¯¹è¯å’Œä»»åŠ¡
    # - smart: é«˜æ™ºèƒ½æ¨¡å¼ï¼Œå¤æ‚æ¨ç†å’Œåˆ›æ„ä»»åŠ¡
    # - coder: ç¼–ç¨‹æ¨¡å¼ï¼Œä»£ç ç”Ÿæˆå’ŒæŠ€æœ¯é—®é¢˜
    # - fast: å¿«é€Ÿæ¨¡å¼ï¼Œæœ¬åœ° Ollama ä½å»¶è¿Ÿå“åº”
    # - vision: è§†è§‰æ¨¡å¼ï¼Œå›¾åƒåˆ†æå’Œå¤šæ¨¡æ€ç†è§£
    #
    # å¦‚æœæœ¬åœ°æ¨¡å‹ä¸å¯ç”¨ï¼Œè‡ªåŠ¨å›é€€åˆ° default
    
    LLM_ROLES = {
        "default": {
            "provider": "openai",
            "api_key": os.getenv("DEFAULT_LLM_API_KEY"),
            "base_url": os.getenv("DEFAULT_LLM_BASE_URL", "https://api.openai.com/v1"),
            "model": os.getenv("DEFAULT_LLM_MODEL", "gpt-3.5-turbo"),
            "timeout": 60,
        },
        "smart": {
            "provider": "openai",
            "api_key": os.getenv("SMART_LLM_API_KEY") or os.getenv("DEFAULT_LLM_API_KEY"),
            "base_url": os.getenv("SMART_LLM_BASE_URL") or os.getenv("DEFAULT_LLM_BASE_URL"),
            "model": os.getenv("SMART_LLM_MODEL", "gpt-4o"),
            "timeout": 120,
        },
        "coder": {
            "provider": os.getenv("CODER_LLM_PROVIDER", "ollama"),
            "model": os.getenv("CODER_LLM_MODEL", "deepseek-coder:6.7b"),
            "host": os.getenv("OLLAMA_HOST", "http://localhost:11434"),
            "timeout": 180,
            # OpenAI fallback (å½“ Ollama ä¸å¯ç”¨æ—¶ä½¿ç”¨)
            "api_key": os.getenv("CODER_LLM_API_KEY") or os.getenv("DEFAULT_LLM_API_KEY"),
            "base_url": os.getenv("CODER_LLM_BASE_URL") or os.getenv("DEFAULT_LLM_BASE_URL"),
        },
        "fast": {
            "provider": os.getenv("FAST_LLM_PROVIDER", "ollama"),
            "model": os.getenv("FAST_LLM_MODEL", "llama3:8b"),
            "host": os.getenv("OLLAMA_HOST", "http://localhost:11434"),
            "timeout": 60,
            # OpenAI fallback
            "api_key": os.getenv("FAST_LLM_API_KEY") or os.getenv("DEFAULT_LLM_API_KEY"),
            "base_url": os.getenv("FAST_LLM_BASE_URL") or os.getenv("DEFAULT_LLM_BASE_URL"),
        },
        "vision": {
            "provider": os.getenv("VISION_LLM_PROVIDER", "gemini"),
            "api_key": os.getenv("VISION_LLM_API_KEY") or os.getenv("GEMINI_API_KEY"),
            "model": os.getenv("VISION_LLM_MODEL", "gemini-1.5-flash"),
            "timeout": 60,
            # OpenAI fallback (å¦‚ GPT-4o)
            "base_url": os.getenv("VISION_LLM_BASE_URL"),
        },
    }
    
    # =========================================
    # âš™ï¸ è¿è¡Œæ—¶å‚æ•° (Runtime Settings)
    # =========================================
    # é›†ä¸­ç®¡ç†å„æ¨¡å—çš„é˜ˆå€¼å’Œè¶…æ—¶ï¼Œé¿å…ç¡¬ç¼–ç 
    
    # æµè§ˆå™¨è‡ªåŠ¨åŒ–
    BROWSER_TASK_TIMEOUT = int(os.getenv("BROWSER_TASK_TIMEOUT", "120"))  # ç§’
    
    # çŸ¥è¯†åº“ RAG
    KNOWLEDGE_CHUNK_SIZE = int(os.getenv("KNOWLEDGE_CHUNK_SIZE", "500"))  # å­—ç¬¦
    KNOWLEDGE_CHUNK_OVERLAP = int(os.getenv("KNOWLEDGE_CHUNK_OVERLAP", "50"))  # å­—ç¬¦
    KNOWLEDGE_MAX_RESULTS = int(os.getenv("KNOWLEDGE_MAX_RESULTS", "5"))  # æ¡
    
    # è¯­éŸ³è¯†åˆ« VAD
    VAD_PAUSE_THRESHOLD = float(os.getenv("VAD_PAUSE_THRESHOLD", "0.8"))  # ç§’
    VAD_MAX_RECORD_SECONDS = int(os.getenv("VAD_MAX_RECORD_SECONDS", "30"))  # ç§’
    
    # å¯¹è¯å†å²
    MAX_HISTORY_MESSAGES = int(os.getenv("MAX_HISTORY_MESSAGES", "30"))  # æ¡ï¼Œé˜²æ­¢ context æº¢å‡º
    
    @staticmethod
    def get_proxy_config():
        """è·å– httpx å…¼å®¹çš„ä»£ç†é…ç½®å­—å…¸"""
        if Config.PROXY_ENABLED and Config.PROXY_URL:
            return {
                "http://": Config.PROXY_URL,
                "https://": Config.PROXY_URL
            }
        return None

    @staticmethod
    def setup_env_proxy():
        """è®¾ç½®ç¯å¢ƒå˜é‡ä»£ç† (ä¾› requests ç­‰åº“ä½¿ç”¨)"""
        if Config.PROXY_ENABLED and Config.PROXY_URL:
            os.environ["http_proxy"] = Config.PROXY_URL
            os.environ["https_proxy"] = Config.PROXY_URL