"""
Jarvis V7.0 Configuration

ç»Ÿä¸€é…ç½®ä¸­å¿ƒï¼Œæ”¯æŒï¼š
- å¤š LLM Provider (OpenAI, Ollama, Gemini)
- è§’è‰²åˆ‡æ¢ (default, smart, coder, fast, vision)
- äººæ ¼ Prompt å®šåˆ¶
- ä»£ç†è®¾ç½®
"""

import os
from dotenv import load_dotenv

# åŠ è½½ .env æ–‡ä»¶
load_dotenv()


class Config:
    """Jarvis ç»Ÿä¸€é…ç½®ç±»"""
    
    # =========================================
    # ğŸ­ äººæ ¼ Prompt é…ç½® (Personality Prompt)
    # =========================================
    # è‡ªå®šä¹‰ Jarvis çš„äººæ ¼ç‰¹å¾ï¼Œå½±å“å¯¹è¯é£æ ¼å’Œè¡Œä¸º
    PERSONALITY_PROMPT = os.getenv("JARVIS_PERSONALITY", """ä½ æ˜¯ Jarvisï¼Œä¸€ä¸ªæ™ºèƒ½ AI åŠ©æ‰‹ã€‚ä½ å‹å¥½ã€æœ‰å¸®åŠ©ï¼Œå¹¶ä¸”èƒ½å¤ŸååŠ©ç”¨æˆ·å®Œæˆå„ç§ä»»åŠ¡ã€‚

ä½ çš„ç‰¹ç‚¹ï¼š
- ç®€æ´æ˜äº†åœ°å›ç­”é—®é¢˜
- åœ¨éœ€è¦æ—¶æä¾›è¯¦ç»†çš„è§£é‡Š
- ä¿æŒå‹å¥½å’Œä¸“ä¸šçš„æ€åº¦
- ä½¿ç”¨ä¸­æ–‡ä¸ç”¨æˆ·äº¤æµï¼ˆé™¤éç”¨æˆ·ä½¿ç”¨å…¶ä»–è¯­è¨€ï¼‰
""")
    
    # ç”¨æˆ·è‡ªå®šä¹‰åç§°ï¼ˆç”¨äºä¸ªæ€§åŒ–ç§°å‘¼ï¼‰
    USER_NAME = os.getenv("JARVIS_USER_NAME", "ä¸»äºº")
    
    # åŠ©æ‰‹åç§°
    ASSISTANT_NAME = os.getenv("JARVIS_ASSISTANT_NAME", "Jarvis")
    
    # =========================================
    # ğŸŒ ç½‘ç»œä¸ä»£ç†é…ç½®
    # =========================================
    PROXY_ENABLED = os.getenv("PROXY_ENABLED", "false").lower() == "true"
    PROXY_URL = os.getenv("PROXY_URL", "http://127.0.0.1:7897")
    
    # =========================================
    # ğŸ¤ è¯­éŸ³ä¸å”¤é†’è¯é…ç½®
    # =========================================
    PICOVOICE_ACCESS_KEY = os.getenv("PICOVOICE_ACCESS_KEY")
    USE_BUILTIN_KEYWORD = os.getenv("USE_BUILTIN_KEYWORD", "true").lower() == "true"
    WAKE_WORD_FILE = os.getenv("WAKE_WORD_FILE", "jarvis.ppn")
    TTS_VOICE = os.getenv("TTS_VOICE", "zh-CN-XiaoxiaoNeural")
    
    # å”¤é†’è¯çµæ•åº¦ (0.0 - 1.0)
    try:
        WAKE_SENSITIVITY = float(os.getenv("WAKE_SENSITIVITY", "0.7"))
        WAKE_SENSITIVITY = max(0.0, min(1.0, WAKE_SENSITIVITY))
    except Exception:
        WAKE_SENSITIVITY = 0.7

    # =========================================
    # ğŸ¤– LLM è§’è‰²é…ç½® (V7.0 ç»Ÿä¸€æ¶æ„)
    # =========================================
    # æ”¯æŒçš„ provider: "openai", "ollama", "gemini"
    # 
    # è§’è‰²è¯´æ˜:
    # - default: å¹³è¡¡æ¨¡å¼ï¼Œæ—¥å¸¸å¯¹è¯å’Œä»»åŠ¡
    # - smart: é«˜æ™ºèƒ½æ¨¡å¼ï¼Œå¤æ‚æ¨ç†å’Œåˆ›æ„ä»»åŠ¡
    # - coder: ç¼–ç¨‹æ¨¡å¼ï¼Œä»£ç ç”Ÿæˆå’ŒæŠ€æœ¯é—®é¢˜
    # - fast: å¿«é€Ÿæ¨¡å¼ï¼Œæœ¬åœ° Ollama ä½å»¶è¿Ÿå“åº”
    # - vision: è§†è§‰æ¨¡å¼ï¼Œå›¾åƒåˆ†æå’Œå¤šæ¨¡æ€ç†è§£
    #
    # å¦‚æœæœ¬åœ°æ¨¡å‹ä¸å¯ç”¨ï¼Œè‡ªåŠ¨å›é€€åˆ° default
    
    LLM_ROLES = {
        "default": {
            "provider": "openai",
            "api_key": os.getenv("DEFAULT_LLM_API_KEY"),
            "base_url": os.getenv("DEFAULT_LLM_BASE_URL", "https://api.openai.com/v1"),
            "model": os.getenv("DEFAULT_LLM_MODEL", "gpt-3.5-turbo"),
            "timeout": 60,
        },
        "smart": {
            "provider": "openai",
            "api_key": os.getenv("SMART_LLM_API_KEY") or os.getenv("DEFAULT_LLM_API_KEY"),
            "base_url": os.getenv("SMART_LLM_BASE_URL") or os.getenv("DEFAULT_LLM_BASE_URL"),
            "model": os.getenv("SMART_LLM_MODEL", "gpt-4o"),
            "timeout": 120,
        },
        "coder": {
            "provider": os.getenv("CODER_LLM_PROVIDER", "ollama"),
            "model": os.getenv("CODER_LLM_MODEL", "deepseek-coder:6.7b"),
            "host": os.getenv("OLLAMA_HOST", "http://localhost:11434"),
            "timeout": 180,
            # OpenAI fallback (å½“ Ollama ä¸å¯ç”¨æ—¶ä½¿ç”¨)
            "api_key": os.getenv("CODER_LLM_API_KEY") or os.getenv("DEFAULT_LLM_API_KEY"),
            "base_url": os.getenv("CODER_LLM_BASE_URL") or os.getenv("DEFAULT_LLM_BASE_URL"),
        },
        "fast": {
            "provider": os.getenv("FAST_LLM_PROVIDER", "ollama"),
            "model": os.getenv("FAST_LLM_MODEL", "llama3:8b"),
            "host": os.getenv("OLLAMA_HOST", "http://localhost:11434"),
            "timeout": 60,
            # OpenAI fallback
            "api_key": os.getenv("FAST_LLM_API_KEY") or os.getenv("DEFAULT_LLM_API_KEY"),
            "base_url": os.getenv("FAST_LLM_BASE_URL") or os.getenv("DEFAULT_LLM_BASE_URL"),
        },
        "vision": {
            "provider": os.getenv("VISION_LLM_PROVIDER", "gemini"),
            "api_key": os.getenv("VISION_LLM_API_KEY") or os.getenv("GEMINI_API_KEY"),
            "model": os.getenv("VISION_LLM_MODEL", "gemini-1.5-flash"),
            "timeout": 60,
            # OpenAI fallback (å¦‚ GPT-4o)
            "base_url": os.getenv("VISION_LLM_BASE_URL"),
        },
    }
    
    # =========================================
    # ğŸ”§ å…¼å®¹æ€§é…ç½® (Legacy - å°†åœ¨æœªæ¥ç‰ˆæœ¬ç§»é™¤)
    # =========================================
    # MODEL_PRESETS: ä¸ºæ—§ç‰ˆ BaseAgent æä¾›å…¼å®¹
    # æ–°ä»£ç è¯·ä½¿ç”¨ LLM_ROLES
    
    @classmethod
    def get_model_presets(cls) -> dict:
        """ç”Ÿæˆå…¼å®¹æ—§ç‰ˆçš„ MODEL_PRESETSï¼ˆä» LLM_ROLES æ´¾ç”Ÿï¼‰"""
        return {
            role: {
                "api_key": config.get("api_key"),
                "base_url": config.get("base_url") or config.get("host"),
                "model": config.get("model"),
            }
            for role, config in cls.LLM_ROLES.items()
        }
    
    # åŠ¨æ€å±æ€§ï¼šå…¼å®¹æ—§ä»£ç 
    MODEL_PRESETS = property(lambda self: Config.get_model_presets())
    
    # Agent è·¯ç”±æ˜ å°„ (å…¼å®¹æ—§ç‰ˆ BaseAgent)
    AGENT_MODEL_MAP = {
        "PythonAgent": "smart",
        "VisionAgent": "vision",
        "WebSurferAgent": "vision"
    }

    @staticmethod
    def get_proxy_config():
        """è·å– httpx å…¼å®¹çš„ä»£ç†é…ç½®å­—å…¸"""
        if Config.PROXY_ENABLED and Config.PROXY_URL:
            return {
                "http://": Config.PROXY_URL,
                "https://": Config.PROXY_URL
            }
        return None

    @staticmethod
    def setup_env_proxy():
        """è®¾ç½®ç¯å¢ƒå˜é‡ä»£ç† (ä¾› requests ç­‰åº“ä½¿ç”¨)"""
        if Config.PROXY_ENABLED and Config.PROXY_URL:
            os.environ["http_proxy"] = Config.PROXY_URL
            os.environ["https_proxy"] = Config.PROXY_URL