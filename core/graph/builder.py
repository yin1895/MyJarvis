"""
Graph Builder for Jarvis V7.0

This module constructs the LangGraph workflow with nodes and edges.
Phase 3 implements persistence and safety interceptor.
"""

from __future__ import annotations

import logging
from typing import Optional, List, Sequence, Union, cast, Any
from functools import lru_cache

from langgraph.graph import StateGraph, START, END
from langgraph.graph.state import CompiledStateGraph
from langgraph.prebuilt import ToolNode, tools_condition
from langgraph.checkpoint.base import BaseCheckpointSaver
from langchain_core.messages import AIMessage, SystemMessage, HumanMessage, ToolMessage, BaseMessage
from langchain_core.tools import BaseTool

from core.graph.state import AgentState, NodeOutput
from core.llm_provider import LLMFactory, RoleType
from config import Config

# Import tools from centralized registry (single source of truth)
from tools import get_native_tools, get_tool_risk_level
from tools.role import ROLE_SWITCH_MARKER

logger = logging.getLogger(__name__)


# ============== Tool Registry ==============

@lru_cache(maxsize=1)
def _build_tool_registry() -> dict[str, BaseTool]:
    """Build tool registry once and cache it."""
    return {tool.name: tool for tool in get_all_tools()}


def get_tool_by_name(name: str) -> Optional[BaseTool]:
    """
    Get a tool instance by its name.
    
    Args:
        name: The tool name
        
    Returns:
        The tool instance or None if not found
    """
    return _build_tool_registry().get(name)


def get_all_tools() -> List[BaseTool]:
    """
    Get all available native tools from centralized registry.
    
    Returns:
        List of LangChain tool instances
    """
    return get_native_tools()


def get_safe_tools() -> List[BaseTool]:
    """
    Get only safe tools (risk_level == "safe").
    
    Returns:
        List of safe tool instances
    """
    return [t for t in get_all_tools() if get_tool_risk_level(t) == "safe"]


def get_dangerous_tools() -> List[BaseTool]:
    """
    Get dangerous tools (risk_level == "dangerous").
    
    Returns:
        List of dangerous tool instances
    """
    return [t for t in get_all_tools() if get_tool_risk_level(t) == "dangerous"]


def check_tool_calls_safety(tool_calls: Sequence[Union[dict, Any]]) -> tuple[bool, List[str]]:
    """
    Check if all tool calls are safe.
    
    Args:
        tool_calls: Sequence of tool call objects (dict or ToolCall) from AIMessage
        
    Returns:
        Tuple of (all_safe: bool, dangerous_tool_names: List[str])
    """
    dangerous_tools = []
    
    for call in tool_calls:
        # Support both dict and ToolCall objects
        tool_name = call.get("name", "") if isinstance(call, dict) else getattr(call, "name", "")
        tool = get_tool_by_name(tool_name)
        
        if tool is None:
            # Unknown tool - treat as dangerous
            dangerous_tools.append(tool_name)
        elif get_tool_risk_level(tool) == "dangerous":
            dangerous_tools.append(tool_name)
    
    return len(dangerous_tools) == 0, dangerous_tools


# ============== System Prompt ==============

def get_system_prompt(mode: str = "text", role: str = "default") -> str:
    """
    Generate dynamic system prompt based on interaction mode and role.
    
    æ ¹æ®äº¤äº’æ¨¡å¼ï¼ˆè¯­éŸ³/æ–‡å­—ï¼‰å’Œå½“å‰è§’è‰²åŠ¨æ€ç”Ÿæˆ system promptï¼Œ
    ç¡®ä¿è¯­éŸ³æ¨¡å¼ä¸‹è¾“å‡ºç®€æ´ã€æ–‡å­—æ¨¡å¼ä¸‹å¯ä»¥è¯¦ç»†ã€‚
    
    Args:
        mode: "voice" æˆ– "text"ï¼Œå½±å“è¾“å‡ºé£æ ¼çº¦æŸ
        role: LLM è§’è‰²ï¼Œå¦‚ "default", "smart", "coder", "vision", "fast"
        
    Returns:
        å®Œæ•´çš„ system prompt å­—ç¬¦ä¸²
    """
    personality = Config.PERSONALITY
    base = personality.get("base", {})
    voice_cfg = personality.get("voice_mode", {})
    text_cfg = personality.get("text_mode", {})
    role_traits = personality.get("roles", {})
    
    # åŸºç¡€äººæ ¼
    name = base.get("name", "Jarvis")
    trait = base.get("trait", "ç®€æ´ã€ä¸“ä¸šã€å‹å¥½")
    language = base.get("language", "ä¸­æ–‡")
    
    prompt_parts = [
        f"ä½ æ˜¯ {name}ï¼Œä¸€ä¸ªæ™ºèƒ½ AI åŠ©æ‰‹ã€‚",
        f"ä½ çš„ç‰¹ç‚¹ï¼š{trait}",
        f"ä½¿ç”¨{language}ä¸ç”¨æˆ·äº¤æµã€‚",
    ]
    
    # è§’è‰²ç‰¹å®šäººæ ¼
    if role in role_traits:
        prompt_parts.append(f"\nã€å½“å‰è§’è‰²æ¨¡å¼ã€‘{role}: {role_traits[role]}")
    
    # äº¤äº’æ¨¡å¼çº¦æŸï¼ˆæ ¸å¿ƒå·®å¼‚ç‚¹ï¼‰
    if mode == "voice":
        style = voice_cfg.get("style", "æåº¦ç®€æ´ï¼Œ1-2å¥è¯")
        rules = voice_cfg.get("rules", [])
        prompt_parts.append(f"\nã€è¯­éŸ³æ¨¡å¼ - æå…¶é‡è¦ã€‘\né£æ ¼è¦æ±‚ï¼š{style}")
        if rules:
            prompt_parts.append("å¿…é¡»éµå®ˆçš„è§„åˆ™ï¼š")
            for rule in rules:
                prompt_parts.append(f"- {rule}")
        # æ­£åä¾‹å¯¹æ¯”
        bad = voice_cfg.get("example_bad")
        good = voice_cfg.get("example_good")
        if bad and good:
            prompt_parts.append(f"\nâŒ ä¸è¦è¿™æ ·å›ç­”ï¼š\"{bad}\"")
            prompt_parts.append(f"âœ… è¦è¿™æ ·å›ç­”ï¼š\"{good}\"")
    else:
        # æ–‡å­—æ¨¡å¼
        style = text_cfg.get("style", "æ¸…æ™°å‡†ç¡®ï¼Œå¯ä»¥é€‚å½“è¯¦ç»†")
        rules = text_cfg.get("rules", [])
        prompt_parts.append(f"\nã€æ–‡å­—æ¨¡å¼ã€‘\né£æ ¼ï¼š{style}")
        if rules:
            for rule in rules:
                prompt_parts.append(f"- {rule}")
    
    # å·¥å…·è¯´æ˜
    tool_instructions = """
ã€å¯ç”¨å·¥å…·ã€‘
å®‰å…¨å·¥å…·ï¼ˆè‡ªåŠ¨æ‰§è¡Œï¼‰ï¼šsystem_control, memory_operation, knowledge_query, vision_analyze, switch_role
å±é™©å·¥å…·ï¼ˆéœ€ç¡®è®¤ï¼‰ï¼šfile_operation, shell_execute, python_interpreter, browser_navigate, knowledge_ingest

æ ¹æ®ç”¨æˆ·éœ€æ±‚é€‰æ‹©åˆé€‚çš„å·¥å…·ï¼Œä¸éœ€è¦å·¥å…·æ—¶ç›´æ¥å›ç­”ã€‚"""
    
    prompt_parts.append(tool_instructions)
    
    return "\n".join(prompt_parts)


# Legacy constant for backward compatibility
DEFAULT_SYSTEM_PROMPT = get_system_prompt()


# ============== Graph Nodes ==============

async def state_updater_node(state: AgentState) -> NodeOutput:
    """
    Post-tool node that checks for role switch markers in tool results.
    
    This node runs after tools execute and before returning to chatbot.
    It detects ROLE_SWITCH_MARKER in ToolMessage content and updates
    the current_role in state accordingly.
    
    Args:
        state: Current agent state
        
    Returns:
        Updated state with new current_role if switch detected
    """
    messages = state.get("messages", [])
    current_role = state.get("current_role", "default")
    
    # Check recent messages for role switch marker
    # Only look at last few messages to avoid old matches
    for msg in reversed(messages[-5:]):
        # Check if this is a ToolMessage from switch_role
        msg_name = getattr(msg, 'name', '')
        if msg_name == 'switch_role':
            content = str(getattr(msg, 'content', ''))
            if ROLE_SWITCH_MARKER in content:
                # Extract new role from marker
                for line in content.split('\n'):
                    if line.startswith(ROLE_SWITCH_MARKER):
                        new_role = line.split(':')[1].strip()
                        if new_role and new_role != current_role:
                            logger.info(f"Role switch detected: {current_role} -> {new_role}")
                            return {"current_role": new_role}
                break
    
    # No role switch detected
    return {}


def _sanitize_messages_for_gemini(messages: List[BaseMessage]) -> List[BaseMessage]:
    """
    æ¸…ç†æ¶ˆæ¯å†å²ä»¥ç¬¦åˆ Gemini API çš„ä¸¥æ ¼è¦æ±‚ã€‚
    
    Gemini API å¯¹æ¶ˆæ¯åºåˆ—æœ‰ç‰¹æ®Šè¦æ±‚ï¼š
    1. function call (AIMessage with tool_calls) åå¿…é¡»ç´§è·Ÿ function response (ToolMessage)
    2. ä¸èƒ½æœ‰è¿ç»­çš„ AI æ¶ˆæ¯ï¼ˆé™¤éæ˜¯ tool response åï¼‰
    3. æ¶ˆæ¯åºåˆ—å¿…é¡»ä»¥ user message æˆ– function response å¼€å§‹ï¼ˆç›¸å¯¹äºä¸Šä¸€ä¸ª AI turnï¼‰
    
    æ­¤å‡½æ•°é€šè¿‡ä»¥ä¸‹ç­–ç•¥ç¡®ä¿å…¼å®¹æ€§ï¼š
    - ç§»é™¤å­¤ç«‹çš„ tool_callsï¼ˆæ²¡æœ‰å¯¹åº” ToolMessage çš„ AIMessage.tool_callsï¼‰
    - ç¡®ä¿ tool_calls å’Œ ToolMessage é…å¯¹å®Œæ•´
    
    Args:
        messages: åŸå§‹æ¶ˆæ¯åˆ—è¡¨
        
    Returns:
        æ¸…ç†åçš„æ¶ˆæ¯åˆ—è¡¨ï¼Œç¬¦åˆ Gemini API è¦æ±‚
    """
    if not messages:
        return messages
    
    sanitized = []
    i = 0
    
    while i < len(messages):
        msg = messages[i]
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯å¸¦æœ‰ tool_calls çš„ AIMessage
        if isinstance(msg, AIMessage) and getattr(msg, 'tool_calls', None):
            tool_calls = msg.tool_calls
            tool_call_ids = {tc.get('id') or tc.id for tc in tool_calls if hasattr(tc, 'id') or isinstance(tc, dict)}
            
            # æŸ¥æ‰¾åç»­çš„ ToolMessageï¼Œç¡®ä¿æ‰€æœ‰ tool_calls éƒ½æœ‰å¯¹åº”çš„å“åº”
            j = i + 1
            found_tool_messages = []
            found_ids = set()
            
            while j < len(messages):
                next_msg = messages[j]
                if isinstance(next_msg, ToolMessage):
                    tool_call_id = getattr(next_msg, 'tool_call_id', None)
                    if tool_call_id in tool_call_ids:
                        found_tool_messages.append(next_msg)
                        found_ids.add(tool_call_id)
                        j += 1
                        continue
                # é‡åˆ°é ToolMessage æˆ–ä¸åŒ¹é…çš„ ToolMessageï¼Œåœæ­¢æœç´¢
                break
            
            # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰ tool_calls éƒ½æœ‰å¯¹åº”çš„ ToolMessage
            if found_ids == tool_call_ids and len(found_ids) > 0:
                # å®Œæ•´çš„ tool call åºåˆ—ï¼Œä¿ç•™
                sanitized.append(msg)
                sanitized.extend(found_tool_messages)
                i = j
            else:
                # ä¸å®Œæ•´çš„ tool call åºåˆ—
                # åˆ›å»ºä¸€ä¸ªæ²¡æœ‰ tool_calls çš„æ–° AIMessageï¼Œåªä¿ç•™ content
                if msg.content:
                    # ä¿ç•™æ–‡æœ¬å†…å®¹ï¼Œç§»é™¤ tool_calls
                    clean_msg = AIMessage(content=msg.content)
                    sanitized.append(clean_msg)
                    logger.debug(f"Sanitized incomplete tool_calls from AIMessage")
                # è·³è¿‡å­¤ç«‹çš„ ToolMessage
                i = j if j > i + 1 else i + 1
        else:
            # æ™®é€šæ¶ˆæ¯ï¼Œç›´æ¥ä¿ç•™
            sanitized.append(msg)
            i += 1
    
    # æœ€åæ£€æŸ¥ï¼šç¡®ä¿ä¸ä»¥ ToolMessage ç»“å°¾ï¼ˆé™¤éåé¢ç´§è·Ÿ AI å“åº”ï¼‰
    # Gemini è¦æ±‚æœ€åä¸€æ¡æ¶ˆæ¯å¿…é¡»æ˜¯ user æˆ– AIï¼ˆé tool_callsï¼‰
    while sanitized and isinstance(sanitized[-1], ToolMessage):
        logger.debug("Removing trailing ToolMessage for Gemini compatibility")
        sanitized.pop()
    
    return sanitized


async def chatbot_node(state: AgentState) -> NodeOutput:
    """
    Main chatbot node that processes user messages and generates responses.
    
    This node:
    1. Gets the current LLM based on the role in state
    2. Binds all available tools to the LLM
    3. Prepends a system message based on interaction_mode and role
    4. Truncates message history to avoid context overflow
    5. Invokes the LLM with the message history
    6. Returns the response (may contain tool calls)
    
    Args:
        state: The current agent state containing messages and metadata
        
    Returns:
        A dict with the new message(s) to append to state
    """
    messages = state.get("messages", [])
    role_str = state.get("current_role") or "default"
    mode = state.get("interaction_mode") or "text"  # é»˜è®¤æ–‡å­—æ¨¡å¼
    role = cast(RoleType, role_str)
    
    # Get the LLM for the current role
    llm = LLMFactory.create(role)
    
    # Bind tools to LLM
    tools = get_all_tools()
    llm_with_tools = llm.bind_tools(tools)
    
    # Generate dynamic system prompt based on mode and role
    system_prompt = get_system_prompt(mode=mode, role=role_str)
    
    # Filter out old system messages to avoid confusion
    filtered_messages = [m for m in messages if not isinstance(m, SystemMessage)]
    
    # ğŸ”§ æ¶ˆæ¯æˆªæ–­ï¼šé¿å… context è¶…å‡ºé™åˆ¶
    # ä¿ç•™æœ€è¿‘çš„ N æ¡æ¶ˆæ¯ï¼ˆå¯é€šè¿‡ Config é…ç½®ï¼‰
    MAX_HISTORY_MESSAGES = getattr(Config, 'MAX_HISTORY_MESSAGES', 30)
    if len(filtered_messages) > MAX_HISTORY_MESSAGES:
        # ä¿ç•™æœ€è¿‘çš„æ¶ˆæ¯ï¼Œç¡®ä¿æœ€åä¸€æ¡æ˜¯ç”¨æˆ·æ¶ˆæ¯
        filtered_messages = filtered_messages[-MAX_HISTORY_MESSAGES:]
        logger.info(f"Truncated message history to {MAX_HISTORY_MESSAGES} messages")
    
    # ğŸ”§ Gemini å…¼å®¹æ€§å¤„ç†ï¼šæ¸…ç†ä¸å®Œæ•´çš„ tool_calls åºåˆ—
    # Gemini API è¦æ±‚ï¼šfunction call åå¿…é¡»ç´§è·Ÿ function response
    # å¦‚æœå†å²æ¶ˆæ¯ä¸­æœ‰å­¤ç«‹çš„ tool_callsï¼ˆæ²¡æœ‰å¯¹åº”çš„ ToolMessageï¼‰ï¼Œä¼šå¯¼è‡´é”™è¯¯
    if role_str in ("vision", "smart") or "gemini" in str(getattr(llm, 'model', '')).lower():
        filtered_messages = _sanitize_messages_for_gemini(filtered_messages)
    
    messages_to_send = [SystemMessage(content=system_prompt)] + filtered_messages
    
    logger.debug(f"Invoking LLM with {len(messages_to_send)} messages, mode={mode}, role={role_str}")
    
    # Invoke the LLM asynchronously with error handling
    try:
        response: AIMessage = await llm_with_tools.ainvoke(messages_to_send)
    except Exception as e:
        logger.error(f"LLM invocation failed: {e}")
        # è¿”å›é”™è¯¯æ¶ˆæ¯è€Œä¸æ˜¯å´©æºƒ
        error_msg = f"æŠ±æ­‰ï¼ŒAI æ¨¡å‹è°ƒç”¨å¤±è´¥ï¼š{str(e)[:100]}"
        return {"messages": [AIMessage(content=error_msg)]}
    
    # æ£€æŸ¥ç©ºå“åº”
    if not response.content and not response.tool_calls:
        logger.warning("LLM returned empty response")
        return {"messages": [AIMessage(content="æŠ±æ­‰ï¼Œæˆ‘æ²¡æœ‰æ”¶åˆ°æœ‰æ•ˆçš„å“åº”ã€‚è¯·é‡è¯•æˆ–æ£€æŸ¥ç½‘ç»œè¿æ¥ã€‚")]}
    
    logger.debug(f"LLM response: {str(response.content)[:100]}...")
    
    # Return the response to be added to messages via the reducer
    return {"messages": [response]}


def create_graph(
    role: RoleType = "default",
    system_prompt: Optional[str] = None,
    checkpointer: Optional[Any] = None,
    interrupt_before_tools: bool = True
) -> CompiledStateGraph:
    """
    Create and compile a new LangGraph workflow with tool support.
    
    This creates a graph with:
    - chatbot node: LLM with bound tools
    - tools node: ToolNode for executing tool calls
    - Routing: chatbot -> tools (if tool_calls) -> chatbot, or chatbot -> END
    - Optional persistence via checkpointer
    - Optional interrupt before tools for safety
    
    Graph flow:
        START -> chatbot -> tools_condition -> tools -> chatbot
                                           -> END
    
    Args:
        role: Default LLM role for the graph
        system_prompt: Optional custom system prompt
        checkpointer: Optional SQLite or memory checkpointer for persistence
        interrupt_before_tools: If True, interrupt before tool execution for safety check
        
    Returns:
        A compiled LangGraph StateGraph ready for execution
        
    Example:
        >>> from langgraph.checkpoint.sqlite import SqliteSaver
        >>> checkpointer = SqliteSaver.from_conn_string("data/state.db")
        >>> graph = create_graph(checkpointer=checkpointer, interrupt_before_tools=True)
    """
    # Get all tools
    tools = get_all_tools()
    
    # Create the state graph with our AgentState schema
    workflow = StateGraph(AgentState)
    
    # Add the chatbot node
    workflow.add_node("chatbot", chatbot_node)
    
    # Add the tools node using prebuilt ToolNode
    tool_node = ToolNode(tools=tools)
    workflow.add_node("tools", tool_node)
    
    # Add state updater node (runs after tools, updates role if needed)
    workflow.add_node("state_updater", state_updater_node)
    
    # Define the graph edges
    # START -> chatbot
    workflow.add_edge(START, "chatbot")
    
    # chatbot -> tools (if tool_calls) or END
    # tools_condition routes to "tools" if there are tool calls, otherwise to END
    workflow.add_conditional_edges(
        "chatbot",
        tools_condition,
    )
    
    # tools -> state_updater -> chatbot (loop back after tool execution)
    workflow.add_edge("tools", "state_updater")
    workflow.add_edge("state_updater", "chatbot")
    
    # Compile options
    compile_kwargs: dict[str, Any] = {}
    
    if checkpointer is not None:
        compile_kwargs["checkpointer"] = checkpointer
    
    if interrupt_before_tools:
        # Interrupt before tools node for safety inspection
        compile_kwargs["interrupt_before"] = ["tools"]
    
    # Compile the graph
    compiled = workflow.compile(**compile_kwargs)
    
    logger.info(f"Graph compiled with {len(tools)} tools, "
                f"checkpointer={'enabled' if checkpointer else 'disabled'}, "
                f"interrupt={'enabled' if interrupt_before_tools else 'disabled'}")
    
    return compiled


# Pre-compiled default graph instance for convenience
# This is lazily evaluated when first accessed
_default_graph: Optional[CompiledStateGraph] = None


def get_graph() -> CompiledStateGraph:
    """
    Get the default compiled graph instance (singleton).
    
    Returns:
        The compiled default graph
    """
    global _default_graph
    if _default_graph is None:
        _default_graph = create_graph()
    return _default_graph


# Export a default graph instance
# Note: This is created at import time for convenience
# For custom configurations, use create_graph() instead
graph = None  # Will be lazily initialized


def init_graph() -> CompiledStateGraph:
    """
    Initialize and return the global graph instance.
    
    Call this at application startup to ensure the graph is ready.
    
    Returns:
        The initialized compiled graph
    """
    global graph
    if graph is None:
        graph = create_graph()
    return graph
